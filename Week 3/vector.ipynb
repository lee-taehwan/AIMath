{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 4.]\n"
     ]
    }
   ],
   "source": [
    "vector1 = np.array([1,2])\n",
    "vector2 = np.array([3,4])\n",
    "vector3 = np.array([5,6])\n",
    "print(sum([vector1, vector2, vector3]) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[3. 4.]\n"
     ]
    }
   ],
   "source": [
    "vectors = np.stack([vector1, vector2, vector3])\n",
    "print(vectors)\n",
    "print(vectors.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(xs):\n",
    "    return sum([x * x for x in xs])\n",
    "\n",
    "def gradient(f, xs, h=1e-6):\n",
    "    derivs = []\n",
    "    for i in range(len(xs)):\n",
    "        xs_ = np.copy(xs)\n",
    "        xs_[i] += h\n",
    "        fx2 = f(xs_)\n",
    "        xs_[i] -= 2*h\n",
    "        fx1 = f(xs_)\n",
    "        derivs.append((fx2 - fx1) / (2*h))\n",
    "    return np.array(derivs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.45832554 -2.64151458] [-2.91665108 -5.28302916]\n",
      "[-1.16666043 -2.11321166] [-2.33332086 -4.22642333]\n",
      "[-0.93332834 -1.69056933] [-1.86665669 -3.38113866]\n",
      "[-0.74666268 -1.35245547] [-1.49332535 -2.70491093]\n",
      "[-0.59733014 -1.08196437] [-1.19466028 -2.16392874]\n",
      "[-0.47786411 -0.8655715 ] [-0.95572822 -1.731143  ]\n",
      "[-0.38229129 -0.6924572 ] [-0.76458258 -1.3849144 ]\n",
      "[-0.30583303 -0.55396576] [-0.61166606 -1.10793152]\n",
      "[-0.24466643 -0.44317261] [-0.48933285 -0.88634521]\n",
      "[-0.19573314 -0.35453809] [-0.39146628 -0.70907617]\n",
      "[-0.15658651 -0.28363047] [-0.31317302 -0.56726094]\n",
      "[-0.12526921 -0.22690437] [-0.25053842 -0.45380875]\n",
      "[-0.10021537 -0.1815235 ] [-0.20043074 -0.363047  ]\n",
      "[-0.08017229 -0.1452188 ] [-0.16034459 -0.2904376 ]\n",
      "[-0.06413784 -0.11617504] [-0.12827567 -0.23235008]\n",
      "[-0.05131027 -0.09294003] [-0.10262054 -0.18588006]\n",
      "[-0.04104821 -0.07435203] [-0.08209643 -0.14870405]\n",
      "[-0.03283857 -0.05948162] [-0.06567714 -0.11896324]\n",
      "[-0.02627086 -0.0475853 ] [-0.05254171 -0.09517059]\n",
      "[-0.02101669 -0.03806824] [-0.04203337 -0.07613647]\n",
      "[-0.01681335 -0.03045459] [-0.0336267  -0.06090918]\n",
      "[-0.01345068 -0.02436367] [-0.02690136 -0.04872734]\n",
      "[-0.01076054 -0.01949094] [-0.02152109 -0.03898187]\n",
      "[-0.00860843 -0.01559275] [-0.01721687 -0.0311855 ]\n",
      "[-0.00688675 -0.0124742 ] [-0.0137735 -0.0249484]\n",
      "[-0.0055094  -0.00997936] [-0.0110188  -0.01995872]\n",
      "[-0.00440752 -0.00798349] [-0.00881504 -0.01596698]\n",
      "[-0.00352601 -0.00638679] [-0.00705203 -0.01277358]\n",
      "[-0.00282081 -0.00510943] [-0.00564162 -0.01021886]\n",
      "[-0.00225665 -0.00408755] [-0.0045133  -0.00817509]\n",
      "[-0.00180532 -0.00327004] [-0.00361064 -0.00654007]\n",
      "[-0.00144426 -0.00261603] [-0.00288851 -0.00523206]\n",
      "[-0.0011554  -0.00209282] [-0.00231081 -0.00418565]\n",
      "[-0.00092432 -0.00167426] [-0.00184865 -0.00334852]\n",
      "[-0.00073946 -0.00133941] [-0.00147892 -0.00267881]\n",
      "[-0.00059157 -0.00107153] [-0.00118313 -0.00214305]\n",
      "[-0.00047325 -0.00085722] [-0.00094651 -0.00171444]\n",
      "[-0.0003786  -0.00068578] [-0.00075721 -0.00137155]\n",
      "[-0.00030288 -0.00054862] [-0.00060576 -0.00109724]\n",
      "[-0.00024231 -0.0004389 ] [-0.00048461 -0.00087779]\n",
      "[-0.00019384 -0.00035112] [-0.00038769 -0.00070224]\n",
      "[-0.00015508 -0.00028089] [-0.00031015 -0.00056179]\n",
      "[-0.00012406 -0.00022472] [-0.00024812 -0.00044943]\n",
      "[-9.92484931e-05 -1.79772167e-04] [-0.0001985  -0.00035954]\n",
      "[-7.93987945e-05 -1.43817733e-04] [-0.0001588  -0.00028764]\n",
      "[-6.35190356e-05 -1.15054187e-04] [-0.00012704 -0.00023011]\n",
      "[-5.08152285e-05 -9.20433494e-05] [-0.00010163 -0.00018409]\n",
      "[-4.06521828e-05 -7.36346795e-05] [-8.13043656e-05 -1.47269359e-04]\n",
      "[-3.25217462e-05 -5.89077436e-05] [-6.50434925e-05 -1.17815487e-04]\n",
      "[-2.60173970e-05 -4.71261949e-05] [-5.20347940e-05 -9.42523898e-05]\n",
      "[-2.08139176e-05 -3.77009559e-05]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.random.randn(2) * 3\n",
    "xi = x0\n",
    "step_size = 1e-1\n",
    "xis = [x0]\n",
    "for i in range(50):\n",
    "    g = gradient(sum_of_squares, xi)\n",
    "    print(xi, g)\n",
    "    xi = xi - g * step_size\n",
    "    xis.append(xi)\n",
    "print(xi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
